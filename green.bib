
@article{varasteh_survey_2017,
	title = {Server Consolidation Techniques in Virtualized Data Centers: A Survey},
	volume = {11},
	doi = {10.1109/JSYST.2015.2458273},
	pages = {772--783},
	number = {2},
	journaltitle = {{IEEE} Systems Journal},
	author = {Varasteh, A. and Goudarzi, M.},
	date = {2017-06},
	keywords = {Bandwidth, cloud computing, Cloud computing, cloud data centers, computer centres, Cooling, data center, Degradation, Energy consumption, energy consumption reduction, energy efficiency, environmental impacts, file servers, Heuristic algorithms, physical machines, power aware computing, resource allocation, Resource management, server consolidation, server consolidation techniques, Servers, virtual machine live migration, virtual machines, virtualisation, virtualization, virtualization technologies, virtualized data centers},
	file = {paper.pdf:/home/leon/Informatik/Green/Resources/paper.pdf:application/pdf}
}

@report{brown_report_2008,
	location = {Berkeley, {CA}},
	title = {Report to Congress on Server and Data Center Energy Efficiency: Public Law 109-431},
	abstract = {{\textless}p{\textgreater}This report was prepared in response to the request from Congress stated in Public Law 109-431 (H.R. 5646), "An Act to Study and Promote the Use of Energy Efficient Computer Servers in the United States." This report assesses current trends in energy use and energy costs of data centers and servers in the U.S. (especially Federal government facilities) and outlines existing and emerging opportunities fo improved energy efficiency. It also makes recommendations for pursuing these energy-efficiency opportunities broadly across the country though the use of information and incentive-based programs.{\textless}/p{\textgreater}{\textless}p{\textgreater}Findings from this report include:{\textless}/p{\textgreater}{\textless}ul{\textgreater}{\textless}li{\textgreater}An estimate that data centers consumed about 61 billion kilowatt-hours ({kWh}) in 2006, roughly 1.5\% of total U.S. electricity consumption, or about \$4.5 billion in electricity costs.{\textless}/li{\textgreater}{\textless}li{\textgreater}Federal servers and data centers alone account for approximately 6 billion {kWh} (10\%) of this electricity consumption, or about \$4.5 billion in electricity costs.{\textless}/li{\textgreater}{\textless}li{\textgreater}Assuming current trends continue, in 5 years the national energy consumption by servers and data centers is expected to nearly double, to nearly 100 billion {kWh}.{\textless}/li{\textgreater}{\textless}li{\textgreater}Existing technologies and strategies could reduce typical server energy use by an estimated 25\% - even greater energy savings are possible with advanced technologies.{\textless}/li{\textgreater}{\textless}li{\textgreater}Assuming state-of-the-art energy efficiency practices are implemented throughout U.S. data centers, this projected energy use can be reduced by up to 55\% compared to current efficiency trends.{\textless}/li{\textgreater}{\textless}/ul{\textgreater}{\textless}p{\textgreater}This report makes several recommendations for policies to achieve this savings potential. Among these recommendations are standarized permormance measurement for data centers and their equipment, leadership on energy efficiency in federal data centers, a private sector energy challenge, information on best practices, and further research and development on energy efficiency technologies and practices.{\textless}/p{\textgreater}},
	author = {Brown, Richard E. and Masanet, Eric R. and Nordman, Bruce and Tschudi, William F. and Shehabi, Arman and Stanley, John and Koomey, Jonathan G. and Sartor, Dale A. and Chan, Peter T.},
	date = {2008-06},
	keywords = {combined heat and power, computers, data centers, energy forecasting, energy star, information technology, servers},
	file = {energy_efficiency_in_data_centers.pdf:/home/leon/Informatik/Green/Resources/energy_efficiency_in_data_centers.pdf:application/pdf}
}

@online{google_cloud_2019,
	title = {Cloud Computing Services},
	url = {https://cloud.google.com/},
	titleaddon = {Google Cloud},
	author = {{Google}},
	date = {2019-12},
	langid = {english},
	file = {Snapshot:/home/leon/Zotero/storage/AIGM32DM/cloud.google.com.html:text/html}
}

@online{amazon_ec2_2019,
	title = {Amazon Elastic Compute Cloud Documentation},
	url = {https://docs.aws.amazon.com/ec2/index.html},
	titleaddon = {Amazon {EC}2},
	author = {{Amazon}},
	date = {2019-12},
	langid = {english},
	file = {Amazon Elastic Compute Cloud Documentation:/home/leon/Zotero/storage/YDQMILE9/index.html:text/html}
}

@online{microsoft_azure_2019,
	title = {Microsoft Azure Cloud Computing Platform \& Services},
	url = {https://azure.microsoft.com/en-us/},
	author = {{Microsoft}},
	date = {2019-12},
	langid = {english},
	file = {Snapshot:/home/leon/Zotero/storage/ULAPVEQ3/en-us.html:text/html}
}

@article{barroso_epc_2007,
	title = {The Case for Energy-Proportional Computing},
	volume = {40},
	issn = {1558-0814},
	doi = {10.1109/MC.2007.443},
	abstract = {Energy-proportional designs would enable large energy savings in servers, potentially doubling their efficiency in real-life use. Achieving energy proportionality will require significant improvements in the energy usage profile of every system component, particularly the memory and disk subsystems.},
	pages = {33--37},
	number = {12},
	journaltitle = {Computer},
	author = {Barroso, Luiz André and Hölzle, Urs},
	date = {2007-12},
	keywords = {Computer crashes, Computer networks, Databases, disc storage, disk subsystem, Embedded computing, energy usage profile, energy-proportional computing, energy-proportional design, file servers, green computing, High performance computing, laptop computer, laptop computers, Large-scale systems, memory subsystem, Network servers, power aware computing, server energy saving, Voltage, Web and internet services, Web server},
	file = {energy_proportional_computing.pdf:/home/leon/Informatik/Green/Resources/energy_proportional_computing.pdf:application/pdf}
}

@article{buyya_energy_efficient_2010,
	title = {Energy-Efficient Management of Data Center Resources for Cloud Computing: A Vision, Architectural Elements, and Open Challenges},
	journaltitle = {{PDPTA}},
	author = {Buyya, Rajkumar and Beloglazov, Anton and Abawajy, Jemal},
	date = {2010-06},
	file = {energy_efficient_management.pdf:/home/leon/Informatik/Green/Resources/energy_efficient_management.pdf:application/pdf}
}

@inproceedings{felter_performance_vm_2015,
	title = {An updated performance comparison of virtual machines and Linux containers},
	doi = {10.1109/ISPASS.2015.7095802},
	abstract = {Cloud computing makes extensive use of virtual machines because they permit workloads to be isolated from one another and for the resource usage to be somewhat controlled. In this paper, we explore the performance of traditional virtual machine ({VM}) deployments, and contrast them with the use of Linux containers. We use {KVM} as a representative hypervisor and Docker as a container manager. Our results show that containers result in equal or better performance than {VMs} in almost all cases. Both {VMs} and containers require tuning to support I/Ointensive applications. We also discuss the implications of our performance results for future cloud architectures.},
	eventtitle = {2015 {IEEE} International Symposium on Performance Analysis of Systems and Software ({ISPASS})},
	pages = {171--172},
	booktitle = {2015 {IEEE} International Symposium on Performance Analysis of Systems and Software ({ISPASS})},
	author = {Felter, Wes and Ferreira, Alexandre and Rajamony, Ram and Rubio, Juan},
	date = {2015-03},
	note = {{ISSN}: null},
	keywords = {cloud architectures, cloud computing, container manager, Containers, Docker, Hardware, {KVM}, Linux, Linux containers, Random access memory, representative hypervisor, Servers, Throughput, virtual machines, Virtual machining},
	file = {IEEE Xplore Abstract Record:/home/leon/Zotero/storage/E7NKQ7RP/citations.html:text/html;IEEE Xplore Full Text PDF:/home/leon/Zotero/storage/FEY9EBCI/Felter et al. - 2015 - An updated performance comparison of virtual machi.pdf:application/pdf}
}

@article{popek_vm_1974,
	title = {Formal Requirements for Virtualizable Third Generation Architectures},
	volume = {17},
	issn = {0001-0782},
	doi = {10.1145/361011.361073},
	abstract = {Virtual machine systems have been implemented on a limited number of third generation computer systems, e.g. {CP}-67 on the {IBM} 360/67. From previous empirical studies, it is known that certain third generation computer systems, e.g. the {DEC} {PDP}-10, cannot support a virtual machine system. In this paper, model of a third-generation-like computer system is developed. Formal techniques are used to derive precise sufficient conditions to test whether such an architecture can support virtual machines.},
	pages = {412--421},
	number = {7},
	journaltitle = {Commun. {ACM}},
	author = {Popek, Gerald J. and Goldberg, Robert P.},
	date = {1974-07},
	keywords = {abstract model, formal requirements, hypervisor, operating system, proof, sensitive instruction, third generation architecture, virtual machine, virtual machine monitor, virtual memory},
	file = {vm_formal_requirements.pdf:/home/leon/Informatik/Green/Resources/vm_formal_requirements.pdf:application/pdf}
}

@article{smith_architecture_2005,
	title = {The architecture of virtual machines},
	volume = {38},
	issn = {1558-0814},
	doi = {10.1109/MC.2005.173},
	abstract = {A virtual machine can support individual processes or a complete system depending on the abstraction level where virtualization occurs. Some {VMs} support flexible hardware usage and software isolation, while others translate from one instruction set to another. Virtualizing a system or component -such as a processor, memory, or an I/O device - at a given abstraction level maps its interface and visible resources onto the interface and resources of an underlying, possibly different, real system. Consequently, the real system appears as a different virtual system or even as multiple virtual systems. Interjecting virtualizing software between abstraction layers near the {HW}/{SW} interface forms a virtual machine that allows otherwise incompatible subsystems to work together. Further, replication by virtualization enables more flexible and efficient and efficient use of hardware resources.},
	pages = {32--38},
	number = {5},
	journaltitle = {Computer},
	author = {Smith, J.E. and Ravi Nair},
	date = {2005-05},
	keywords = {abstraction level, application binary interface, application program interfaces, application programming interface, Application software, computer architecture, Computer architecture, computer architectures, Computer interfaces, disk storage, formal specification, Hardware, hardware resources, hardware-software codesign, {HW}/{SW} interface, instruction set architecture, Instruction sets, Microprocessors, multiprocessing systems, multiprogramming, Operating systems, real system, software systems, virtual machine architecture, virtual machines, Virtual machining, Virtual manufacturing, virtualization technology, {VM} taxonomy, Voice mail},
	file = {IEEE Xplore Abstract Record:/home/leon/Zotero/storage/D2S7JI5Q/1430629.html:text/html;vm_architecture.pdf:/home/leon/Informatik/Green/Resources/vm_architecture.pdf:application/pdf}
}

@online{xen,
	title = {Xen},
	url = {https://xenproject.org/},
	titleaddon = {Xen Project},
	author = {{Xen Project}},
    date = {2019-12},
	urldate = {2019-12-04},
	langid = {american},
	file = {Snapshot:/home/leon/Zotero/storage/3T6S7UEX/xenproject.org.html:text/html}
}

@online{kvm,
	title = {{KVM}},
	url = {https://www.linux-kvm.org},
	author = {{KVM Project}},
    date = {2019-12},
	urldate = {2019-12-05},
	file = {KVM:/home/leon/Zotero/storage/ST3AISEE/Main_Page.html:text/html}
}

@online{virtualbox,
	title = {Oracle {VM} {VirtualBox}},
	url = {https://www.virtualbox.org/},
	author = {{VirtualBox}},
    date = {2019-12},
	urldate = {2019-12-10},
	file = {Oracle VM VirtualBox:/home/leon/Zotero/storage/RUDB5CNB/www.virtualbox.org.html:text/html}
}

@inproceedings{clark_migration_2005,
	location = {Berkeley, {CA}, {USA}},
	title = {Live Migration of Virtual Machines},
	series = {{NSDI}'05},
	abstract = {Migrating operating system instances across distinct physical hosts is a useful tool for administrators of data centers and clusters: It allows a clean separation between hard-ware and software, and facilitates fault management, load balancing, and low-level system maintenance. By carrying out the majority of migration while {OSes} continue to run, we achieve impressive performance with minimal service downtimes; we demonstrate the migration of entire {OS} instances on a commodity cluster, recording service downtimes as low as 60ms. We show that that our performance is sufficient to make live migration a practical tool even for servers running interactive loads. In this paper we consider the design options for migrating {OSes} running services with liveness constraints, focusing on data center and cluster environments. We introduce and analyze the concept of writable working set, and present the design, implementation and evaluation of high-performance {OS} migration built on top of the Xen {VMM}.},
	pages = {273--286},
	booktitle = {Proceedings of the 2Nd Conference on Symposium on Networked Systems Design \& Implementation - Volume 2},
	publisher = {{USENIX} Association},
	author = {Clark, Christopher and Fraser, Keir and Hand, Steven and Hansen, Jacob Gorm and Jul, Eric and Limpach, Christian and Pratt, Ian and Warfield, Andrew},
	date = {2005}
}

@inproceedings{theimer_migration_1985,
	location = {New York, {NY}, {USA}},
	title = {Preemptable Remote Execution Facilities for the V-system},
	isbn = {978-0-89791-174-0},
	url = {http://doi.acm.org/10.1145/323647.323629},
	doi = {10.1145/323647.323629},
	series = {{SOSP} '85},
	pages = {2--12},
	booktitle = {Proceedings of the Tenth {ACM} Symposium on Operating Systems Principles},
	publisher = {{ACM}},
	author = {Theimer, Marvin M. and Lantz, Keith A. and Cheriton, David R.},
	urldate = {2019-12-10},
	date = {1985}
}

@article{gmach_resource_2009,
	title = {Resource pool management: Reactive versus proactive or let’s be friends},
	volume = {53},
	doi = {10.1016/j.comnet.2009.08.011},
	pages = {2905--2922},
	journaltitle = {Computer Networks},
	author = {Gmach, D. and Rolia, Jerry and Cherkasova, Ludmila and Kemper, Alfons},
	date = {2009},
	file = {resource_pool_management_reactive_versus_proactive.pdf:/home/leon/Informatik/GN/Resources/resource_pool_management_reactive_versus_proactive.pdf:application/pdf}
}

@inproceedings{sawyer_calculating_2004,
	location = {West Kingston, {RI}, {USA}},
	title = {Calculating total power requirements for data centers},
	volume = {70},
	pages = {80--90},
	booktitle = {Amer. Power Convers.},
	author = {Sawyer, Richard L.},
	date = {2004}
}

@article{masanet_estimating_2011,
	title = {Estimating the Energy Use and Efficiency Potential of U.S. Data Centers},
	volume = {99},
	issn = {1558-2256},
	doi = {10.1109/JPROC.2011.2155610},
	abstract = {Data centers are a significant and growing component of electricity demand in the United States. This paper presents a bottom-up model that can be used to estimate total data center electricity demand within a region as well as the potential electricity savings associated with energy efficiency improvements. The model is applied to estimate 2008 U.S. data center electricity demand and the technical potential for electricity savings associated with major measures for {IT} devices and infrastructure equipment. Results suggest that 2008 demand was approximately 69 billion kilowatt hours (1.8\% of 2008 total U.S. electricity sales) and that it may be technically feasible to reduce this demand by up to 80\% (to 13 billion kilowatt hours) through aggressive pursuit of energy efficiency measures. Measure-level savings estimates are provided, which shed light on the relative importance of different measures at the national level. Measures applied to servers are found to have the greatest contribution to potential savings.},
	pages = {1440--1453},
	number = {8},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Masanet, Eric R. and Brown, Richard E. and Shehabi, Arman and Koomey, Jonathan G. and Nordman, Bruce},
	date = {2011-08},
	keywords = {bottom-up model, computer centres, data centers, Data centers, Data models, Data systems, Electric variables measurement, electricity demand, electricity savings, energy conservation, energy demand modeling, energy efficiency, Energy efficiency, Energy management, information technology, {IT} devices, load forecasting, Mathematical model, power aware computing, Power systems planning, Program processors, Supply and demand},
	file = {IEEE Xplore Abstract Record:/home/leon/Zotero/storage/XJ38WUHH/5934691.html:text/html;IEEE Xplore Full Text PDF:/home/leon/Zotero/storage/4UEUB4MY/Masanet et al. - 2011 - Estimating the Energy Use and Efficiency Potential.pdf:application/pdf}
}

@article{koomey_worldwide_2008,
	title = {Worldwide electricity used in data centers},
	volume = {3},
	issn = {1748-9326},
	doi = {10.1088/1748-9326/3/3/034008},
	abstract = {The direct electricity used by data centers has become an important issue in recent years as demands for new Internet services (such as search, music downloads, video-on-demand, social networking, and telephony) have become more widespread. This study estimates historical electricity used by data centers worldwide and regionally on the basis of more detailed data than were available for previous assessments, including electricity used by servers, data center communications, and storage equipment. Aggregate electricity use for data centers doubled worldwide from 2000 to 2005. Three quarters of this growth was the result of growth in the number of the least expensive (volume) servers. Data center communications and storage equipment each contributed about 10\% of the growth. Total electricity use grew at an average annual rate of 16.7\% per year, with the Asia Pacific region (without Japan) being the only major world region with growth significantly exceeding that average. Direct electricity used by information technology equipment in data centers represented about 0.5\% of total world electricity consumption in 2005. When electricity for cooling and power distribution is included, that figure is about 1\%. Worldwide data center power demand in 2005 was equivalent (in capacity terms) to about seventeen 1000 {MW} power plants.},
	pages = {034008},
	number = {3},
	journaltitle = {Environmental Research Letters},
	shortjournal = {Environ. Res. Lett.},
	author = {Koomey, Jonathan G.},
	date = {2008-07},
	langid = {english},
	file = {IOP Full Text PDF:/home/leon/Zotero/storage/D9V4Q3HP/Koomey - 2008 - Worldwide electricity used in data centers.pdf:application/pdf}
}

@online{iea_analysis_2019,
	title = {Data centres and data transmission networks – Tracking Buildings – Analysis},
	url = {https://www.iea.org/reports/tracking-buildings/data-centres-and-data-transmission-networks},
	abstract = {Tracking Buildings - Analysis and key findings. A report by the International Energy Agency.},
	author = {{IEA}},
	urldate = {2019-12-15},
	date = {2019},
	langid = {british},
	file = {Snapshot:/home/leon/Zotero/storage/JHBBDIXE/data-centres-and-data-transmission-networks.html:text/html}
}

@inproceedings{meng_traffic_aware_2010,
	title = {Improving the Scalability of Data Center Networks with Traffic-aware Virtual Machine Placement},
	doi = {10.1109/INFCOM.2010.5461930},
	abstract = {The scalability of modern data centers has become a practical concern and has attracted significant attention in recent years. In contrast to existing solutions that require changes in the network architecture and the routing protocols, this paper proposes using traffic-aware virtual machine ({VM}) placement to improve the network scalability. By optimizing the placement of {VMs} on host machines, traffic patterns among {VMs} can be better aligned with the communication distance between them, e.g. {VMs} with large mutual bandwidth usage are assigned to host machines in close proximity. We formulate the {VM} placement as an optimization problem and prove its hardness. We design a two-tier approximate algorithm that efficiently solves the {VM} placement problem for very large problem sizes. Given the significant difference in the traffic patterns seen in current data centers and the structural differences of the recently proposed data center architectures, we further conduct a comparative analysis on the impact of the traffic patterns and the network architectures on the potential performance gain of traffic-aware {VM} placement. We use traffic traces collected from production data centers to evaluate our proposed {VM} placement algorithm, and we show a significant performance improvement compared to existing general methods that do not take advantage of traffic patterns and data center network characteristics.},
	eventtitle = {2010 Proceedings {IEEE} {INFOCOM}},
	pages = {1--9},
	booktitle = {2010 Proceedings {IEEE} {INFOCOM}},
	author = {Meng, Xiaoqiao and Pappas, Vasileios and Zhang, Li},
	date = {2010-03},
	note = {{ISSN}: 0743-166X},
	keywords = {Algorithm design and analysis, Bandwidth, computer centres, data center network architecture, data center network characteristics, network scalability, Pattern analysis, Performance analysis, routing protocols, Routing protocols, Scalability, Telecommunication traffic, traffic patterns, traffic-aware virtual machine placement, two-tier approximate algorithm, virtual machines, Virtual machining, Virtual manufacturing, Voice mail},
	file = {IEEE Xplore Abstract Record:/home/leon/Zotero/storage/NRSPHBK5/5461930.html:text/html;IEEE Xplore Full Text PDF:/home/leon/Zotero/storage/VGXAMTWW/Meng et al. - 2010 - Improving the Scalability of Data Center Networks .pdf:application/pdf}
}

@inproceedings{tang_thermal_aware_2007,
	title = {Thermal-aware task scheduling for data centers through minimizing heat recirculation},
	doi = {10.1109/CLUSTR.2007.4629225},
	abstract = {The thermal environment of data centers plays a significant role in affecting the energy efficiency and the reliability of data center operation. A dominant problem associated with cooling data centers is the recirculation of hot air from the equipment outlets to their inlets, causing the appearance of hot spots and an uneven inlet temperature distribution. Heat is generated due to the execution of tasks, and it varies according to the power profile of a task. We are looking into the prospect of assigning the incoming tasks around the data center in such a way so as to make the inlet temperatures as even as possible; this will allow for considerable cooling power savings. Based on our previous research work on characterizing the heat recirculation in terms of cross-interference coefficients, we propose a task scheduling algorithm for homogeneous data centers, called {XInt}, that minimizes the inlet temperatures, and leads to minimal heat recirculation and minimal cooling energy cost for data center operation. We verify, through both theoretical formalization and simulation, that minimizing heat recirculation will result in the best cooling energy efficiency. {XInt} leads to an inlet temperature distribution that is 2degC to 5degC lower than other approaches, and achieves about 20\%-30\% energy savings at moderate data center utilization rates. {XInt} also consistently achieves the best energy efficiency compared to another recirculation minimized algorithm, {MinHR}.},
	eventtitle = {2007 {IEEE} International Conference on Cluster Computing},
	pages = {129--138},
	booktitle = {2007 {IEEE} International Conference on Cluster Computing},
	author = {Tang, Qinghui and Gupta, Sandeep K. S. and Varsamopoulos, Georgios},
	date = {2007-09},
	note = {{ISSN}: 2168-9253},
	keywords = {Blades, Cooling, cross-interference coefficients, data centers, data handling, heat recirculation, Heating, homogeneous data centers, hot air recirculation, Power demand, Program processors, recirculation minimized algorithm, Servers, task analysis, task scheduling algorithm, Temperature distribution, thermal environments, thermal-aware task scheduling},
	file = {IEEE Xplore Abstract Record:/home/leon/Zotero/storage/VXADWD6Q/4629225.html:text/html;IEEE Xplore Full Text PDF:/home/leon/Zotero/storage/FZ36PZTM/Tang et al. - 2007 - Thermal-aware task scheduling for data centers thr.pdf:application/pdf}
}

@article{greenberg_vl2_2011,
	title = {{VL}2: A Scalable and Flexible Data Center Network},
	volume = {54},
	issn = {0001-0782},
	doi = {10.1145/1897852.1897877},
	pages = {95--104},
	number = {3},
	journaltitle = {Commun. {ACM}},
	author = {Greenberg, Albert and Hamilton, James R. and Jain, Navendu and Kandula, Srikanth and Kim, Changhoon and Lahiri, Parantap and Maltz, David A. and Patel, Parveen and Sengupta, Sudipta},
	date = {2011-03}
}
